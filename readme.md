#This projct is all about a 2 way system where:

1.  If a person is using a spoken language to communicate then his/her 
speech is translated to sign language performedby a 3d avatar.

2.  If a person is using sign-language to communicate his/her signs will be
translated to voice and spoken by the 3d avatar

It is a challenging project with a great opportunity to learn!!!

#Development Process:

Dealing with the 1st system first, i.e translating spoken language into sign language!

Steps Involved:
1. Developing speech recognition Component
    1. taking user speech as input 
    2. Input Processing
    3. recognizing the speech using advanced NLP techniques 
    4. converting recognized speech to text for further processing
    5. Testing the speech recognition component
2. Developing the text to Sign Language Translation Component
    (Creating a dictionary that maps words/phrases to corresponding sign language gestures)
3. Create a 3d Avatar and animating it
    1. Development of an avatar 
    2. Adding animations to the avatar for each sign language gesture
    3. Creating an animation blueprint
    4. Fine-tune the animations and ensurte accurate hand and finger movements using a control rig
4. Real time Integration
    1. Blueprint Scripting: to trigger animations based on the Sign Language Translation Component
    2. creating API to convert Speech to text and text to sign gestures in real time 
5. Testing and Iterating
    1. testing the System with various inputs to ensure accuracy and smooth performance
    2. Optimizing the system for optimal performance
    3. Making improvements based on testing feedback


.................................IN DEVELOPMENT ......................................................